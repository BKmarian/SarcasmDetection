{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.1; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the 'c:\\users\\sichi\\appdata\\local\\programs\\python\\python38\\python.exe -m pip install --upgrade pip' command.\n",
      "c:\\users\\sichi\\pycharmprojects\\nlpproiect\\venv\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\users\\sichi\\pycharmprojects\\nlpproiect\\venv\\lib\\site-packages\\numpy\\.libs\\libopenblas.NOIJJG62EMASZI6NYURL6JBKM4EVBGM7.gfortran-win_amd64.dll\n",
      "c:\\users\\sichi\\pycharmprojects\\nlpproiect\\venv\\lib\\site-packages\\numpy\\.libs\\libopenblas.QVLO2T66WEPI7JZ63PS3HMOHFEY472BC.gfortran-win_amd64.dll\n",
      "c:\\users\\sichi\\pycharmprojects\\nlpproiect\\venv\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydotplus in c:\\users\\sichi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (2.0.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.1 in c:\\users\\sichi\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pydotplus) (2.4.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install pydotplus\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from gensim.models import KeyedVectors\n",
    "from keras import Input, Model\n",
    "from keras.callbacks import TensorBoard, ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n",
    "from keras_preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional,GRU\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from keras.layers import  Flatten\n",
    "from sklearn.metrics import  confusion_matrix,f1_score,accuracy_score\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/clean/train.csv')\n",
    "cv = pd.read_csv('data/clean/cv.csv')\n",
    "test = pd.read_csv('data/clean/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train['comment'] = train['comment'].astype(str)\n",
    "cv['comment'] = cv['comment'].astype(str)\n",
    "test['comment'] = test['comment'].astype(str)\n",
    "\n",
    "train['author'] = train['author'].astype(str)\n",
    "cv['author'] = cv['author'].astype(str)\n",
    "test['author'] = test['author'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149433\n"
     ]
    }
   ],
   "source": [
    "t = Tokenizer()\n",
    "t.fit_on_texts(train['comment'].values)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "encoded_comments_train = t.texts_to_sequences(train['comment'])\n",
    "encoded_comments_cv = t.texts_to_sequences(cv['comment'])\n",
    "encoded_comments_test = t.texts_to_sequences(test['comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "max_sent_length = 70\n",
    "padded_comments_train = pad_sequences(encoded_comments_train, maxlen=max_sent_length, padding='post')\n",
    "padded_comments_cv = pad_sequences(encoded_comments_cv, maxlen=max_sent_length, padding='post')\n",
    "padded_comments_test = pad_sequences(encoded_comments_test, maxlen=max_sent_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "y_train = train['label'].values\n",
    "y_cv = cv['label'].values\n",
    "y_test = test['label'].values\n",
    "\n",
    "y_train = to_categorical(y_train, num_classes=2)\n",
    "y_cv = to_categorical(y_cv, num_classes=2)\n",
    "y_test = to_categorical(y_test, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "w2v_model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(149433, 300)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix_w2v = np.zeros((vocab_size, 300))\n",
    "for word, i in t.word_index.items():\n",
    "    try:\n",
    "        embedding_vector = w2v_model[word]\n",
    "    except:\n",
    "        embedding_vector = [0]*300\n",
    "\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix_w2v[i] = embedding_vector\n",
    "\n",
    "embedding_matrix_w2v.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "reduce_lr = ReduceLROnPlateau(monitor='val_f1_m',\n",
    "                              mode = 'max',\n",
    "                              factor=0.5,\n",
    "                              patience=5,\n",
    "                              min_lr=0.0001,\n",
    "                              verbose=10)\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"lstm_model.h5\",\n",
    "                               monitor=\"val_f1_m\",\n",
    "                               mode=\"max\",\n",
    "                               save_best_only = True,\n",
    "                               verbose=1)\n",
    "\n",
    "earlystop = EarlyStopping(monitor = 'val_f1_m',\n",
    "                            mode=\"max\",\n",
    "                            min_delta = 0,\n",
    "                            patience = 5,\n",
    "                            verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "12500/12500 [==============================] - ETA: 0s - loss: 0.5838 - f1_m: 0.6844 - accuracy: 0.6844\n",
      "Epoch 00001: val_f1_m improved from -inf to 0.70703, saving model to lstm_model.h5\n",
      "12500/12500 [==============================] - 9396s 751ms/step - loss: 0.5838 - f1_m: 0.6844 - accuracy: 0.6844 - val_loss: 0.5564 - val_f1_m: 0.7070 - val_accuracy: 0.7071 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "12500/12500 [==============================] - ETA: 0s - loss: 0.5495 - f1_m: 0.7148 - accuracy: 0.7148\n",
      "Epoch 00002: val_f1_m improved from 0.70703 to 0.73409, saving model to lstm_model.h5\n",
      "12500/12500 [==============================] - 9560s 765ms/step - loss: 0.5495 - f1_m: 0.7148 - accuracy: 0.7148 - val_loss: 0.5309 - val_f1_m: 0.7341 - val_accuracy: 0.7341 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "12500/12500 [==============================] - ETA: 0s - loss: 0.5347 - f1_m: 0.7264 - accuracy: 0.7264\n",
      "Epoch 00003: val_f1_m improved from 0.73409 to 0.74071, saving model to lstm_model.h5\n",
      "12500/12500 [==============================] - 9613s 769ms/step - loss: 0.5347 - f1_m: 0.7264 - accuracy: 0.7264 - val_loss: 0.5214 - val_f1_m: 0.7407 - val_accuracy: 0.7407 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "12500/12500 [==============================] - ETA: 0s - loss: 0.5229 - f1_m: 0.7349 - accuracy: 0.7349\n",
      "Epoch 00004: val_f1_m improved from 0.74071 to 0.74206, saving model to lstm_model.h5\n",
      "12500/12500 [==============================] - 9379s 750ms/step - loss: 0.5229 - f1_m: 0.7349 - accuracy: 0.7349 - val_loss: 0.5169 - val_f1_m: 0.7421 - val_accuracy: 0.7420 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "12500/12500 [==============================] - ETA: 0s - loss: 0.5114 - f1_m: 0.7427 - accuracy: 0.7427\n",
      "Epoch 00005: val_f1_m improved from 0.74206 to 0.74427, saving model to lstm_model.h5\n",
      "12500/12500 [==============================] - 9362s 749ms/step - loss: 0.5114 - f1_m: 0.7427 - accuracy: 0.7427 - val_loss: 0.5170 - val_f1_m: 0.7443 - val_accuracy: 0.7442 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "12500/12500 [==============================] - ETA: 0s - loss: 0.5050 - f1_m: 0.7471 - accuracy: 0.7471\n",
      "Epoch 00006: val_f1_m improved from 0.74427 to 0.74611, saving model to lstm_model.h5\n",
      "12500/12500 [==============================] - 9037s 723ms/step - loss: 0.5050 - f1_m: 0.7471 - accuracy: 0.7471 - val_loss: 0.5159 - val_f1_m: 0.7461 - val_accuracy: 0.7461 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "12500/12500 [==============================] - ETA: 0s - loss: 0.4966 - f1_m: 0.7526 - accuracy: 0.7526\n",
      "Epoch 00007: val_f1_m did not improve from 0.74611\n",
      "12500/12500 [==============================] - 9200s 736ms/step - loss: 0.4966 - f1_m: 0.7526 - accuracy: 0.7526 - val_loss: 0.5165 - val_f1_m: 0.7452 - val_accuracy: 0.7452 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "12500/12500 [==============================] - ETA: 0s - loss: 0.4891 - f1_m: 0.7570 - accuracy: 0.7570\n",
      "Epoch 00008: val_f1_m did not improve from 0.74611\n",
      "12500/12500 [==============================] - 9129s 730ms/step - loss: 0.4891 - f1_m: 0.7570 - accuracy: 0.7570 - val_loss: 0.5227 - val_f1_m: 0.7431 - val_accuracy: 0.7431 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "12500/12500 [==============================] - ETA: 0s - loss: 0.4840 - f1_m: 0.7610 - accuracy: 0.7610\n",
      "Epoch 00009: val_f1_m did not improve from 0.74611\n",
      "12500/12500 [==============================] - 9179s 734ms/step - loss: 0.4840 - f1_m: 0.7610 - accuracy: 0.7610 - val_loss: 0.5176 - val_f1_m: 0.7458 - val_accuracy: 0.7458 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "12500/12500 [==============================] - ETA: 0s - loss: 0.4802 - f1_m: 0.7637 - accuracy: 0.7637\n",
      "Epoch 00010: val_f1_m did not improve from 0.74611\n",
      "12500/12500 [==============================] - 9197s 736ms/step - loss: 0.4802 - f1_m: 0.7637 - accuracy: 0.7637 - val_loss: 0.5211 - val_f1_m: 0.7436 - val_accuracy: 0.7436 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "12500/12500 [==============================] - ETA: 0s - loss: 0.4766 - f1_m: 0.7659 - accuracy: 0.7659\n",
      "Epoch 00011: val_f1_m improved from 0.74611 to 0.74722, saving model to lstm_model.h5\n",
      "12500/12500 [==============================] - 9206s 737ms/step - loss: 0.4766 - f1_m: 0.7659 - accuracy: 0.7659 - val_loss: 0.5198 - val_f1_m: 0.7472 - val_accuracy: 0.7472 - lr: 0.0010\n",
      "Epoch 12/50\n",
      "12500/12500 [==============================] - ETA: 0s - loss: 0.4731 - f1_m: 0.7680 - accuracy: 0.7680\n",
      "Epoch 00012: val_f1_m did not improve from 0.74722\n",
      "12500/12500 [==============================] - 9274s 742ms/step - loss: 0.4731 - f1_m: 0.7680 - accuracy: 0.7680 - val_loss: 0.5261 - val_f1_m: 0.7441 - val_accuracy: 0.7441 - lr: 0.0010\n",
      "Epoch 13/50\n",
      "12500/12500 [==============================] - ETA: 0s - loss: 0.4703 - f1_m: 0.7697 - accuracy: 0.7697\n",
      "Epoch 00013: val_f1_m did not improve from 0.74722\n",
      "12500/12500 [==============================] - 9337s 747ms/step - loss: 0.4703 - f1_m: 0.7697 - accuracy: 0.7697 - val_loss: 0.5202 - val_f1_m: 0.7456 - val_accuracy: 0.7455 - lr: 0.0010\n",
      "Epoch 14/50\n",
      "12500/12500 [==============================] - ETA: 0s - loss: 0.4686 - f1_m: 0.7709 - accuracy: 0.7709\n",
      "Epoch 00014: val_f1_m did not improve from 0.74722\n",
      "12500/12500 [==============================] - 9294s 743ms/step - loss: 0.4686 - f1_m: 0.7709 - accuracy: 0.7709 - val_loss: 0.5263 - val_f1_m: 0.7433 - val_accuracy: 0.7433 - lr: 0.0010\n",
      "Epoch 15/50\n",
      "12500/12500 [==============================] - ETA: 0s - loss: 0.4639 - f1_m: 0.7739 - accuracy: 0.7739\n",
      "Epoch 00015: val_f1_m did not improve from 0.74722\n",
      "12500/12500 [==============================] - 9318s 745ms/step - loss: 0.4639 - f1_m: 0.7739 - accuracy: 0.7739 - val_loss: 0.5288 - val_f1_m: 0.7436 - val_accuracy: 0.7436 - lr: 0.0010\n",
      "Epoch 16/50\n",
      "12500/12500 [==============================] - ETA: 0s - loss: 0.4651 - f1_m: 0.7734 - accuracy: 0.7734\n",
      "Epoch 00016: val_f1_m did not improve from 0.74722\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "12500/12500 [==============================] - 9385s 751ms/step - loss: 0.4651 - f1_m: 0.7734 - accuracy: 0.7734 - val_loss: 0.5278 - val_f1_m: 0.7417 - val_accuracy: 0.7417 - lr: 0.0010\n",
      "Epoch 00016: early stopping\n",
      "[[  65  223 1086 ...    0    0    0]\n",
      " [  13    4  154 ...    0    0    0]\n",
      " [  52    9 1984 ...    0    0    0]\n",
      " ...\n",
      " [ 103   22 1329 ...    0    0    0]\n",
      " [  68    7   31 ...    0    0    0]\n",
      " [  59  118   11 ...    0    0    0]]\n",
      "[[0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " ...\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n",
      "3125/3125 [==============================] - 664s 213ms/step - loss: 0.5237 - f1_m: 0.7456 - accuracy: 0.7456\n",
      "[0.5236588716506958, 0.7456472516059875, 0.7456267476081848]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#print(lstm_model.summary())\n",
    "\n",
    "for SIZE1 in [300]:\n",
    "   # for SIZE2 in [50,100,200,300]:\n",
    "        input_data = Input(shape=(max_sent_length,), name='main_input')\n",
    "        embedding_layer = Embedding(vocab_size, 300, weights=[embedding_matrix_w2v], trainable=False)(input_data)\n",
    "        dropout_1 = Dropout(0.3)(embedding_layer)\n",
    "        bilstm = Bidirectional(LSTM(SIZE1))(dropout_1)\n",
    "        dropout_2 = Dropout(0.3)(bilstm)\n",
    "       # gru = Bidirectional(LSTM(SIZE2))(dropout_2)\n",
    "       # dropout_3 = Dropout(0.3)(gru)\n",
    "        max_2 = Flatten()(dropout_2)\n",
    "        flatten = Flatten()(max_2)\n",
    "        out = Dense(2, activation='softmax', name='fully_connected')(flatten)\n",
    "\n",
    "        lstm_model = Model(inputs=[input_data], outputs=[out])\n",
    "\n",
    "       # print(lstm_model.summary())\n",
    "\n",
    "        #keras.utils.vis_utils.pydot = pyd\n",
    "        #plot_model(lstm_model, to_file='lstm_model.png')\n",
    "\n",
    "        #%%\n",
    "\n",
    "        lstm_model.compile(loss='binary_crossentropy',optimizer='adam',metrics=[f1_score,accuracy_score])\n",
    "\n",
    "        lstm_model_h1 = lstm_model.fit(padded_comments_train, y_train,\n",
    "                       batch_size=64,\n",
    "                       epochs=50,\n",
    "                       verbose=1, callbacks=[checkpoint, earlystop, reduce_lr], #tensorboard\n",
    "                       validation_data=(padded_comments_cv, y_cv))\n",
    "\n",
    "        #%%\n",
    "\n",
    "        print(padded_comments_cv)\n",
    "        print(y_cv)\n",
    "\n",
    "        #%%\n",
    "\n",
    "        score_1 = lstm_model.evaluate(padded_comments_test, y_test)\n",
    "        print(score_1)\n",
    "\n",
    "        #%%\n",
    "\n",
    "#         cnf_mat = confusion_matrix(np.argmax(y_test, axis=1), np.argmax(lstm_model.predict(padded_comments_test), axis=1))\n",
    "\n",
    "#         print(cnf_mat)\n",
    "#         sns.heatmap(cnf_mat, annot=True, fmt='g', linewidths=.5, xticklabels=['Predicted 0', 'Predicted 1'],\n",
    "#                     yticklabels=['Actual 0', 'Actual 1'])\n",
    "\n",
    "        #%%\n",
    "\n",
    "        plt.plot(lstm_model_h1.history['f1_m'][1:])\n",
    "        plt.plot(lstm_model_h1.history['val_f1_m'][1:])\n",
    "        plt.title('Model metric')\n",
    "        plt.ylabel('F1 metric')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train','Validation'], loc='upper left')\n",
    "        plt.savefig('bilstm_f1_{}.png'.format(SIZE1))\n",
    "        #plt.show()\n",
    "        plt.clf()\n",
    "        #%%\n",
    "\n",
    "        plt.plot(lstm_model_h1.history['loss'][1:])\n",
    "        plt.plot(lstm_model_h1.history['val_loss'][1:])\n",
    "        plt.title('Model Los')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train','Validation'], loc='upper left')\n",
    "        plt.savefig('Bilstm_Loss_{}.png'.format(SIZE1))\n",
    "        #plt.show()\n",
    "        plt.clf()\n",
    "        #%%\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}