{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install pydotplus\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pickle\n",
    "import warnings\n",
    "\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pydotplus as pyd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from gensim.models import KeyedVectors\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping, ReduceLROnPlateau\n",
    "from keras.layers import Dense, Input, Flatten, Embedding, Conv1D, MaxPooling1D\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score,f1_score\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/sarcasm/data/clean/train.csv')\n",
    "cv = pd.read_csv('../input/sarcasm/data/clean/cv.csv')\n",
    "test = pd.read_csv('../input/sarcasm/data/clean/test.csv')\n",
    "\n",
    "train.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train['comment'] = train['comment'].astype(str)\n",
    "cv['comment'] = cv['comment'].astype(str)\n",
    "test['comment'] = test['comment'].astype(str)\n",
    "\n",
    "train['author'] = train['author'].astype(str)\n",
    "cv['author'] = cv['author'].astype(str)\n",
    "test['author'] = test['author'].astype(str)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "t = Tokenizer()\n",
    "t.fit_on_texts(train['comment'].values)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "print(vocab_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "encoded_comments_train = t.texts_to_sequences(train['comment'])\n",
    "encoded_comments_cv = t.texts_to_sequences(cv['comment'])\n",
    "encoded_comments_test = t.texts_to_sequences(test['comment'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "max_length = 512\n",
    "padded_comments_train = pad_sequences(encoded_comments_train, maxlen=max_length, padding='post')\n",
    "padded_comments_cv = pad_sequences(encoded_comments_cv, maxlen=max_length, padding='post')\n",
    "padded_comments_test = pad_sequences(encoded_comments_test, maxlen=max_length, padding='post')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_train = train['label'].values\n",
    "y_cv = cv['label'].values\n",
    "y_test = test['label'].values\n",
    "\n",
    "y_train = to_categorical(y_train, num_classes=2)\n",
    "y_cv = to_categorical(y_cv, num_classes=2)\n",
    "y_test = to_categorical(y_test, num_classes=2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "w2v_model = KeyedVectors.load_word2vec_format('../input/wordvec/GoogleNews-vectors-negative300.bin', binary=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix_w2v = np.zeros((vocab_size, 300))\n",
    "for word, i in t.word_index.items():\n",
    "    try:\n",
    "        embedding_vector = w2v_model[word]\n",
    "    except:\n",
    "        embedding_vector = [0]*300\n",
    "\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix_w2v[i] = embedding_vector\n",
    "\n",
    "embedding_matrix_w2v.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "reduce_lr = ReduceLROnPlateau(monitor='val_f1_m',\n",
    "                              mode = 'max',\n",
    "                              factor=0.5,\n",
    "                              patience=5,\n",
    "                              min_lr=0.0001,\n",
    "                              verbose=10)\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"model_01.h5\",\n",
    "                               monitor=\"val_f1_m\",\n",
    "                               mode=\"max\",\n",
    "                               save_best_only = True,\n",
    "                               verbose=1)\n",
    "\n",
    "earlystop = EarlyStopping(monitor = 'val_f1_m',\n",
    "                            mode=\"max\",\n",
    "                            min_delta = 0,\n",
    "                            patience = 5,\n",
    "                            verbose=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model 1: Baseline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "input_data = Input(shape=(max_length,), name='main_input')\n",
    "embedding_layer = Embedding(vocab_size, 300, weights=[embedding_matrix_w2v], trainable=False)(input_data)\n",
    "conv_1 = Conv1D(filters=50, kernel_size=4, activation='relu')(embedding_layer)\n",
    "max_1 = MaxPooling1D(pool_size=2)(conv_1)\n",
    "conv_2 = Conv1D(filters=100, kernel_size=3, activation='relu')(max_1)\n",
    "max_2 = MaxPooling1D(pool_size=2)(conv_2)\n",
    "flatten = Flatten()(max_2)\n",
    "dense = Dense(100, activation='relu', name='fully_connected')(flatten)\n",
    "out = Dense(2, activation='softmax')(dense)\n",
    "\n",
    "model_01 = Model(inputs=[input_data], outputs=[out])\n",
    "\n",
    "print(model_01.summary())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#tensorboard = TensorBoard(log_dir='model_01_{}_{}_{}_{}'.format(filters1,filters2,kernel1,kernel2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "c = tf.keras.optimizers.Adam(lr = 0.0001)\n",
    "model_01.compile(optimizer=c, loss='categorical_crossentropy', metrics=[f1_score, accuracy_score])\n",
    "\n",
    "h1 = model_01.fit(padded_comments_train, y_train,\n",
    "               batch_size=64,\n",
    "               epochs=50,\n",
    "               verbose=1, callbacks=[ checkpoint, earlystop, reduce_lr],  #tensorboard\n",
    "               validation_data=(padded_comments_cv, y_cv))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "score_1 = model_01.evaluate(padded_comments_test, y_test)\n",
    "print(score_1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cnf_mat = confusion_matrix(np.argmax(y_test, axis=1), np.argmax(model_01.predict(padded_comments_test), axis=1))\n",
    "\n",
    "print(cnf_mat)\n",
    "sns.heatmap(cnf_mat, annot=True, fmt='g', linewidths=.5, xticklabels=['Predicted 0', 'Predicted 1'],\n",
    "            yticklabels=['Actual 0', 'Actual 1'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(h1.history['f1_m'][1:])\n",
    "plt.plot(h1.history['val_f1_m'][1:])\n",
    "plt.title('Model metric')\n",
    "plt.ylabel('F1 metric')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','Validation'], loc='upper left')\n",
    "plt.savefig('CNN_f1_50_4_100_3_512_length.png')\n",
    "plt.clf()\n",
    "\n",
    "#plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(h1.history['loss'][1:])\n",
    "plt.plot(h1.history['val_loss'][1:])\n",
    "plt.title('Model Los')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','Validation'], loc='upper left')\n",
    "plt.savefig('CNN_f1_50_4_100_3_512_length.png')\n",
    "plt.clf()\n",
    "\n",
    "#plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!cd /kaggle/working\n",
    "!ls -l"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1275.229865,
   "end_time": "2022-04-03T20:47:58.588672",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-04-03T20:26:43.358807",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}