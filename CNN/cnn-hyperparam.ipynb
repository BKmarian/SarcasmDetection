{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "execution": {
     "iopub.status.busy": "2022-04-02T09:21:07.435759Z",
     "iopub.execute_input": "2022-04-02T09:21:07.436028Z",
     "iopub.status.idle": "2022-04-02T10:38:59.824336Z",
     "shell.execute_reply.started": "2022-04-02T09:21:07.435999Z",
     "shell.execute_reply": "2022-04-02T10:38:59.822250Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install pydotplus\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-02T14:01:51.533797Z",
     "iopub.execute_input": "2022-04-02T14:01:51.534122Z",
     "iopub.status.idle": "2022-04-02T14:01:52.895430Z",
     "shell.execute_reply.started": "2022-04-02T14:01:51.534036Z",
     "shell.execute_reply": "2022-04-02T14:01:52.894586Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pickle\n",
    "import warnings\n",
    "\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pydotplus as pyd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from gensim.models import KeyedVectors\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping, ReduceLROnPlateau\n",
    "from keras.layers import Dense, Input, Flatten, Embedding, Conv1D, MaxPooling1D\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/sarcasm/data/clean/train.csv')\n",
    "cv = pd.read_csv('../input/sarcasm/data/clean/cv.csv')\n",
    "test = pd.read_csv('../input/sarcasm/data/clean/test.csv')\n",
    "\n",
    "train.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train['comment'] = train['comment'].astype(str)\n",
    "cv['comment'] = cv['comment'].astype(str)\n",
    "test['comment'] = test['comment'].astype(str)\n",
    "\n",
    "train['author'] = train['author'].astype(str)\n",
    "cv['author'] = cv['author'].astype(str)\n",
    "test['author'] = test['author'].astype(str)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "t = Tokenizer()\n",
    "t.fit_on_texts(train['comment'].values)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "print(vocab_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "encoded_comments_train = t.texts_to_sequences(train['comment'])\n",
    "encoded_comments_cv = t.texts_to_sequences(cv['comment'])\n",
    "encoded_comments_test = t.texts_to_sequences(test['comment'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "max_length = 70\n",
    "padded_comments_train = pad_sequences(encoded_comments_train, maxlen=max_length, padding='post')\n",
    "padded_comments_cv = pad_sequences(encoded_comments_cv, maxlen=max_length, padding='post')\n",
    "padded_comments_test = pad_sequences(encoded_comments_test, maxlen=max_length, padding='post')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_train = train['label'].values\n",
    "y_cv = cv['label'].values\n",
    "y_test = test['label'].values\n",
    "\n",
    "y_train = to_categorical(y_train, num_classes=2)\n",
    "y_cv = to_categorical(y_cv, num_classes=2)\n",
    "y_test = to_categorical(y_test, num_classes=2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "w2v_model = KeyedVectors.load_word2vec_format('../input/wordvec/GoogleNews-vectors-negative300.bin', binary=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix_w2v = np.zeros((vocab_size, 300))\n",
    "for word, i in t.word_index.items():\n",
    "    try:\n",
    "        embedding_vector = w2v_model[word]\n",
    "    except:\n",
    "        embedding_vector = [0]*300\n",
    "\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix_w2v[i] = embedding_vector\n",
    "\n",
    "embedding_matrix_w2v.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "reduce_lr = ReduceLROnPlateau(monitor='val_f1_m',\n",
    "                              mode = 'max',\n",
    "                              factor=0.5,\n",
    "                              patience=5,\n",
    "                              min_lr=0.0001,\n",
    "                              verbose=10)\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"model_01.h5\",\n",
    "                               monitor=\"val_f1_m\",\n",
    "                               mode=\"max\",\n",
    "                               save_best_only = True,\n",
    "                               verbose=1)\n",
    "\n",
    "earlystop = EarlyStopping(monitor = 'val_f1_m',\n",
    "                            mode=\"max\",\n",
    "                            min_delta = 0,\n",
    "                            patience = 5,\n",
    "                            verbose=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model 1: Baseline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for filters1 in [50,100,200,300]:\n",
    "   # for filters2 in [50,100,200,300]:\n",
    "        for kernel1 in [3,4]:\n",
    "       #     for kernel2 in [3,4]:\n",
    "                print('Iteration: {} {}'.format(filters1,kernel1))\n",
    "                input_data = Input(shape=(max_length,), name='main_input')\n",
    "                embedding_layer = Embedding(vocab_size, 300, weights=[embedding_matrix_w2v], trainable=False)(input_data)\n",
    "                conv_1 = Conv1D(filters=filters1, kernel_size=kernel1, activation='relu')(embedding_layer)\n",
    "                max_1 = MaxPooling1D(pool_size=2)(conv_1)\n",
    "              #  conv_2 = Conv1D(filters=filters2, kernel_size=kernel2, activation='relu')(max_1)\n",
    "             #   max_2 = MaxPooling1D(pool_size=2)(conv_2)\n",
    "                flatten = Flatten()(max_1) #   max_2\n",
    "                dense = Dense(100, activation='relu', name='fully_connected')(flatten)\n",
    "                out = Dense(2, activation='softmax')(dense)\n",
    "\n",
    "                model_01 = Model(inputs=[input_data], outputs=[out])\n",
    "\n",
    "                print(model_01.summary())\n",
    "\n",
    "                #%%\n",
    "\n",
    "                #keras.utils.vis_utils.pydot = pyd\n",
    "                #plot_model(model_01, to_file='model_01_{}_{}_{}_{}.png'.format(filters1,filters2,kernel1,kernel2))\n",
    "\n",
    "                #%%\n",
    "\n",
    "                #tensorboard = TensorBoard(log_dir='model_01_{}_{}_{}_{}'.format(filters1,filters2,kernel1,kernel2))\n",
    "\n",
    "                #%%\n",
    "\n",
    "                c = tf.keras.optimizers.Adam(lr = 0.0001)\n",
    "                model_01.compile(optimizer=c, loss='categorical_crossentropy', metrics=[f1_score, 'acc'])\n",
    "\n",
    "                h1 = model_01.fit(padded_comments_train, y_train,\n",
    "                               batch_size=64,\n",
    "                               epochs=50,\n",
    "                               verbose=1, callbacks=[ checkpoint, earlystop, reduce_lr],  #tensorboard\n",
    "                               validation_data=(padded_comments_cv, y_cv))\n",
    "\n",
    "                #%%\n",
    "\n",
    "                score_1 = model_01.evaluate(padded_comments_test, y_test)\n",
    "                print(score_1)\n",
    "\n",
    "                #%%\n",
    "\n",
    "#                 cnf_mat = confusion_matrix(np.argmax(y_test, axis=1), np.argmax(model_01.predict(padded_comments_test), axis=1))\n",
    "\n",
    "#                 print(cnf_mat)\n",
    "#                 sns.heatmap(cnf_mat, annot=True, fmt='g', linewidths=.5, xticklabels=['Predicted 0', 'Predicted 1'],\n",
    "#                             yticklabels=['Actual 0', 'Actual 1'])\n",
    "\n",
    "                #%%\n",
    "\n",
    "                plt.plot(h1.history['f1_m'][1:])\n",
    "                plt.plot(h1.history['val_f1_m'][1:])\n",
    "                plt.title('Model metric')\n",
    "                plt.ylabel('F1 metric')\n",
    "                plt.xlabel('epoch')\n",
    "                plt.legend(['train','Validation'], loc='upper left')\n",
    "                plt.savefig('CNN_f1_{}_{}.png'.format(filters1,kernel1))\n",
    "              #  plt.show()\n",
    "\n",
    "                #%%\n",
    "\n",
    "                plt.plot(h1.history['loss'][1:])\n",
    "                plt.plot(h1.history['val_loss'][1:])\n",
    "                plt.title('Model Los')\n",
    "                plt.ylabel('Loss')\n",
    "                plt.xlabel('epoch')\n",
    "                plt.legend(['train','Validation'], loc='upper left')\n",
    "                plt.savefig('CNN_Loss_{}_{}.png'.format(filters1,kernel1))\n",
    "               # plt.show()\n",
    "\n",
    "                #%% md\n",
    "\n",
    "                ## Conclusions\n",
    "\n",
    "                #%%\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!cd /kaggle/working\n",
    "!ls -l"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}